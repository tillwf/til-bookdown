[["index.html", "Table of contents", " Table of contents Data Engineering Machine Learning Interviews Miscellaneous "],["data-engineering.html", "1 Data Engineering", " 1 Data Engineering How to deploy an HuggingFace model API on Google Cloud Function Filter in JOIN vs filter in WHERE "],["how-to-deploy-an-huggingface-model-api-on-google-cloud-function.html", "1.1 How to deploy an HuggingFace model API on Google Cloud Function", " 1.1 How to deploy an HuggingFace model API on Google Cloud Function Create a file main.py like that: from transformers import TFBertForSequenceClassification from transformers import AutoTokenizer import tensorflow as tf model = None tokenizer = None def predict(request): global model global tokenizer if model is None: model = TFBertForSequenceClassification.from_pretrained(&quot;jonaskoenig/xtremedistil-l6-h256-uncased-question-vs-statement-classifier&quot;, cache_dir=&quot;/tmp&quot;) tokenizer = AutoTokenizer.from_pretrained(&quot;jonaskoenig/xtremedistil-l6-h256-uncased-question-vs-statement-classifier&quot;, cache_dir=&quot;/tmp&quot;) params = request.get_json() sentence = params[&quot;description&quot;] predictions = model(**tokenizer(sentence, return_tensors=&quot;tf&quot;)) probabilities = tf.nn.softmax(predictions.logits[0], axis=0).numpy() return { &quot;no_question&quot;: float(probabilities[0]), &quot;question&quot;: float(probabilities[1]) } a file requirements.txt like that: tensorflow==2.12.0 transformers==4.27.4 a Dockerfile like that: FROM python:3.10.6 ENV PYTHONUNBUFFERED 1 EXPOSE 8000 WORKDIR /app COPY requirements.txt ./ RUN pip install --upgrade pip &amp;&amp; \\ pip install -r requirements.txt COPY . ./ ENV PYTHONPATH huggingfastapi CMD [&quot;uvicorn&quot;, &quot;main:app&quot;, &quot;--host&quot;, &quot;0.0.0.0&quot;, &quot;--port&quot;, &quot;8000&quot;] You can now build your image and push it to your registry: docker build -t &lt;image_url&gt; -f Dockerfile . docker push &lt;image_url&gt; and deploy it on Google Cloud Run: gcloud run deploy &lt;image_name&gt; --image &lt;image_url&gt;:latest --region europe-west1 --port 8000 --memory 4Gi "],["filter-in-join-vs-filter-in-where.html", "1.2 Filter in JOIN vs filter in WHERE", " 1.2 Filter in JOIN vs filter in WHERE Init tables library(DBI) db = dbConnect(RSQLite::SQLite(), dbname = &quot;db.sqlite&quot;) CREATE TABLE users AS SELECT 1 AS user_id, DATETIME(&quot;2023-01-01&quot;) AS created_at UNION ALL SELECT 2 AS user_id, DATETIME(&quot;2023-01-02&quot;) AS created_at UNION ALL SELECT 3 AS user_id, DATETIME(&quot;2023-01-03&quot;) AS created_at; CREATE TABLE trackers AS SELECT 1 AS user_id, 1 AS trackable_id, DATETIME(&quot;2023-01-04&quot;) AS created_at UNION ALL SELECT 2 AS user_id, 1 AS trackable_id, DATETIME(&quot;2023-01-02&quot;) AS created_at UNION ALL SELECT 2 AS user_id, 1 AS trackable_id, DATETIME(&quot;2023-01-02&quot;) AS created_at; SELECT * FROM users; Table 1.1: 3 records user_id created_at 1 2023-01-01 00:00:00 2 2023-01-02 00:00:00 3 2023-01-03 00:00:00 SELECT * FROM trackers; Table 1.2: 3 records user_id trackable_id created_at 1 1 2023-01-04 00:00:00 2 1 2023-01-02 00:00:00 2 1 2023-01-02 00:00:00 Simple JOIN SELECT * FROM users u LEFT JOIN trackers t ON u.user_id = t.user_id Table 1.3: 4 records user_id created_at user_id trackable_id created_at 1 2023-01-01 00:00:00 1 1 2023-01-04 00:00:00 2 2023-01-02 00:00:00 2 1 2023-01-02 00:00:00 2 2023-01-02 00:00:00 2 1 2023-01-02 00:00:00 3 2023-01-03 00:00:00 NA NA NA Filter in the WHERE clause SELECT * FROM users u LEFT JOIN trackers t ON u.user_id = t.user_id WHERE DATE(t.created_at) &gt;= DATE(&quot;2023-01-05&quot;, &quot;-10 DAY&quot;) Table 1.4: 3 records user_id created_at user_id trackable_id created_at 1 2023-01-01 00:00:00 1 1 2023-01-04 00:00:00 2 2023-01-02 00:00:00 2 1 2023-01-02 00:00:00 2 2023-01-02 00:00:00 2 1 2023-01-02 00:00:00 You lose the user_id=3 Filter in the JOIN clause SELECT * FROM users u LEFT JOIN trackers t ON u.user_id = t.user_id AND DATE(t.created_at) &gt;= DATE(&quot;2023-01-05&quot;, &quot;-10 DAY&quot;) Table 1.5: 4 records user_id created_at user_id trackable_id created_at 1 2023-01-01 00:00:00 1 1 2023-01-04 00:00:00 2 2023-01-02 00:00:00 2 1 2023-01-02 00:00:00 2 2023-01-02 00:00:00 2 1 2023-01-02 00:00:00 3 2023-01-03 00:00:00 NA NA NA Adding more filters in the JOIN clause SELECT * FROM users u LEFT JOIN trackers t ON u.user_id = t.user_id AND DATE(t.created_at) &gt;= DATE(&quot;2023-01-05&quot;, &quot;-10 DAY&quot;) AND t.created_at BETWEEN DATETIME(u.created_at, &quot;+ 24 HOUR&quot;) AND DATETIME(u.created_at, &quot;+ 48 HOUR&quot;) Table 1.6: 3 records user_id created_at user_id trackable_id created_at 1 2023-01-01 00:00:00 NA NA NA 2 2023-01-02 00:00:00 NA NA NA 3 2023-01-03 00:00:00 NA NA NA Then you can make an accurate count SELECT u.user_id, CAST(COUNT(DISTINCT t.trackable_id) &gt; 0 AS INT) AS has_action FROM users u LEFT JOIN trackers t ON u.user_id = t.user_id AND DATE(t.created_at) &gt;= DATE(&quot;2023-01-05&quot;, &quot;-10 DAY&quot;) AND t.created_at BETWEEN DATETIME(u.created_at, &quot;+ 24 HOUR&quot;) AND DATETIME(u.created_at, &quot;+ 48 HOUR&quot;) GROUP BY u.user_id Table 1.7: 3 records user_id has_action 1 0 2 0 3 0 If we filter on the users in the JOIN clause SELECT u.user_id, CAST(COUNT(DISTINCT t.trackable_id) &gt; 0 AS INT) AS has_action FROM users u LEFT JOIN trackers t ON u.user_id = t.user_id AND DATE(t.created_at) &gt;= DATE(&quot;2023-01-05&quot;, &quot;-10 DAY&quot;) AND t.created_at BETWEEN DATETIME(u.created_at, &quot;+ 24 HOUR&quot;) AND DATETIME(u.created_at, &quot;+ 48 HOUR&quot;) AND u.user_id = 3 GROUP BY u.user_id Table 1.8: 3 records user_id has_action 1 0 2 0 3 0 If we filter on the users in the WHERE clause SELECT u.user_id, CAST(COUNT(DISTINCT t.trackable_id) &gt; 0 AS INT) AS has_action FROM users u LEFT JOIN trackers t ON u.user_id = t.user_id AND DATE(t.created_at) &gt;= DATE(&quot;2023-01-05&quot;, &quot;-10 DAY&quot;) AND t.created_at BETWEEN DATETIME(u.created_at, &quot;+ 24 HOUR&quot;) AND DATETIME(u.created_at, &quot;+ 48 HOUR&quot;) WHERE u.created_at &gt;= &quot;2023-01-03&quot; GROUP BY u.user_id Table 1.9: 1 records user_id has_action 3 0 "],["interviews.html", "2 Interviews", " 2 Interviews Homework Leetcode ML Design Questions "],["homework.html", "2.1 Homework", " 2.1 Homework United Credit - Adult Census Income Adomik - Human or Robot Zenly - Spatial Group Detection Zenly - Point of Interest Detection LeBonCoin - Fake User Detection Vestiaire Collective - User Price/Size Prediction Mirakl - Product Classification Voodoo - LTV Prediction WeMoms - Article Ranking "],["leet-code.html", "2.2 Leet Code", " 2.2 Leet Code Solution Repository 2024 349. Intersection of Two Arrays (Easy) 2022 200. Number of Islands (Medium) 528. Random Pick with Weight (Medium) 408. Valid Word Abbreviation (Easy) 2019 322. Coin Change (Medium) "],["ml-design.html", "2.3 ML Design", " 2.3 ML Design 2024 Click Probability Prediction Design a system which predicts the probability of click on an ad. Voice-powered Medical Assistant As a large step in our AI transformation, we aim to build a voice-powered Medical Assistant for care teams to help them: Summarize the consultation Extract and label key information about the patient Support them during the consultation by suggesting next questions The user magnitude is in the order of 100k with healthcare practitioners spread across various medical specialties. The goal is to release this product in 1 year. The platform is still open-ended and you can make the decisions that seem relevant to you. For all use cases, you can assume that some Data can be collected to support the training of a model. You are part of the Data science team responsible to build the AI blocks of this system. Regression with Large Datasets Context Data Characteristics: 1 million rows, 3000 float-type features, dataset too large for memory. Objective: Implement linear regression. Key Considerations Data Quality: No missing values, outliers need further examination. Multicollinearity: High feature count may introduce multicollinearity, inflating coefficient variance. Approach Dimension Reduction: Use techniques like PCA to reduce feature space and manage multicollinearity. Mini-Batch Optimization: Apply an optimizer like Adam with a mini-batch gradient descent strategy to handle large-scale data efficiently. 2.3.1 Anomaly Detection 2.3.1.1 Context A monitoring system alerts users whenever an anomaly is detected. However, users are reporting excessive alerts. 2.3.1.2 Key Considerations Data Scope: How many time series are monitored? Detection Method: What criteria or methods are used to define an anomaly? Alert Optimization: Can similar alerts be grouped to reduce notifications? Alert Resolution: Should resolved issues prevent future alerts for similar events? 2.3.1.3 Approach Sliding Window Comparison: Use two sliding windows to analyze the difference in mean values. Point vs. Historical Data: Compare the latest point to the mean of the last \\(N\\) points to detect deviations exceeding a threshold, such as \\(\\sigma/2\\), where \\(\\sigma\\) is the standard deviation. "],["questions.html", "2.4 Questions", " 2.4 Questions 2024 M. Data Manipulation in Pandas: Differences between pd.concat, pd.merge, and pd.join Functionality of zip and how it works Best practices for unit testing functions that rely on database data Explanation of the Word2Vec algorithm T. Probability &amp; Simulation: Using a dice to simulate a coin toss Extending to a dice with \\(N\\) sides and another with \\(M\\) sides (where \\(M &gt; N\\)) Calculating the probability of achieving a specified outcome only on the 4th throw: \\(P(X=4)\\) DD. Statistics: Methods to demonstrate linear correlation between two variables "],["machine-learning.html", "3 Machine Learning", " 3 Machine Learning A/B Testing XGBoost "],["ab-testing.html", "3.1 A/B Testing", " 3.1 A/B Testing 3.1.1 How to setup an A/B Test 3.1.1.1 Business Goal Know your business journey -&gt; product sense (customer exposure) Define a success metric (only one is preferable): Mesurable Attributable: link feature to effect Sensitive: not too much variability. ex. retention is too incensitive Timely: measure effect within 2 weeks Define guardrail metrics Latency Take the effect on the first exposure. ex. CTR of the first session only. TODO: Talk about Goodhart’s law 3.1.1.2 Hypothesis Testing Define null hypotheis One tail example: There is no increment on the CTR Two tail example: There is no effect on the CTR The 4 parameters. Only one can be derived by the others Confidence/Significant level $ (Type I error)(False Positive Rate) Statistical power \\((1-\\beta)\\) (1 - Type II errir)(False Negative Rate) Minimal detectable effect / Effect size / Practical Significance For example, you can compute the minimum sample size like so: \\[\\begin{equation} n = \\frac{2\\sigma^2(z_{\\alpha/2}+z_{\\beta})}{\\theta^2} \\tag{3.1} \\end{equation}\\] Hypothesis-Testing-Error-Types Determining a test plan includes balancing the probability of a false positive with that of a false negative. Decreasing the probability of one error increases the probability of another, everything else being equal. The only way to reduce both is to increase the test’s sample size @Manual{ABTesting, title = {A/B Testing Statistics – A Concise Guide for Non-Statisticians}, author = {{Georgi Georgiev}}, url = {https://blog.analytics-toolkit.com/2022/a-b-testing-statistics-a-concise-guide/}, } citep{ABTesting} 3.1.1.3 Define Experimentation Randomizatin unit: how to assign variation, network effect Target Population: visitor/searcher/browser Sample size: imply duration (using (??)) (no less than 2 weeks) 3.1.1.4 Run experimentation Collect the data Follow dashboards 3.1.1.5 Validity check Instrumentation effect: check guardrail metrics External factor: holiday, competition, covid Selection bias: A/A testing Sample ratio mismatch: \\(\\chi^2\\) Novelty/Primacy effect 3.1.1.6 Interpretation Choose statistical test: z-test/t-test/ANOVA etc. Get sources and real justification for this sentence: In real life, you don’t know the variance, but thanks to the big numbers law the t-test formula becomes z-test 3.1.2 Statistical Tests 3.1.2.1 Assumptions Data must be independant Data in each group must be randomly picked Data in each group must be normaly distributed Values are continuous Variance in both groups must be equal 3.1.2.2 T-Test Mean diff: \\[\\begin{equation} \\bar{x_A} - \\bar{x_B} \\end{equation}\\] Pooled std dev: \\[\\begin{equation} s_p^2\\frac{(n_A - 1)\\sigma_A^2 + (n_B - 1)\\sigma_B^2}{n_A + n_B - 2} \\end{equation}\\] Compute t: \\[\\begin{equation} $t = \\frac{\\text{diff of groups avg}}{\\text{standard error of difference}} = \\frac{\\bar{x_A} - \\bar{x_B}}{\\sqrt{s_p^2}\\sqrt{\\frac{1}{n_A}\\frac{1}{n_B}}} \\end{equation}\\] Find t-value for related (usually 0.05) and the degree of freedom (\\(n_A + n_B - 2\\)) scipy.stats.t.ppf(1 - alpha/2, df) Compare the compute t with the t-value. If t &gt; t-value, we reject \\(H_0\\) 3.1.2.3 Z-Test Mean \\[\\begin{equation} \\frac{\\bar{x_T} - \\bar{x_C}}{\\sqrt{\\frac{\\sigma_T^2}{n_T} + \\frac{\\sigma_C^2}{n_C}}} \\end{equation}\\] Proportion \\[\\begin{equation} \\frac{p_T - p_C}{\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{N}}} \\end{equation}\\] with \\[\\begin{equation} \\hat{p} = \\frac{y_C + y_T}{N_C + N_T} \\end{equation}\\] 3.1.2.4 Margin 3.1.3 Question 3.1.3.1 How to conduct A/A test ? 3.1.3.2 Can we be statistically not significant ? A statistically non-significant outcome due to a high p-value is not to be used as evidence that there is no effect or that there is a negative effect. 3.1.3.3 How to handle A/B test sample size with ramp-up or imbalanced variations ? 3.1.3.4 How to handle A/B test parameters with more than 2 variations ? 3.1.3.5 How to compute variance on historical data ? "],["xgboost.html", "3.2 XGBoost", " 3.2 XGBoost "],["miscellaneous.html", "4 Miscellaneous", " 4 Miscellaneous "],["convert-cbz-files-to-pdf.html", "4.1 Convert CBZ files to PDF", " 4.1 Convert CBZ files to PDF 4.1.1 Convertion Install mutool command line: sudo apt install mupdf-tools Then to convert every cbz file of a folder do: for i in *.cbz do echo &quot;$i&quot; mutool convert -O compress -o &quot;`basename &quot;$i&quot; .cbz`&quot;.pdf &quot;$i&quot; done The -0 compress is there to reduce the pdf final size. If you want to find every cbz file recursively and convert them do: find . -name &quot;*.cbz&quot; -exec mutool convert -O compress -o {}.pdf {} \\; or if you want to remove the .cbz: find . -name &quot;*.cbz&quot; -exec sh -c &#39;mutool convert -O compress -o &quot;$(basename &quot;{}&quot; .cbz)&quot;.pdf &quot;{}&quot;&#39; \\ 4.1.2 Convert the pdf to grayscale to reduce its size gs \\ -sOutputFile=output.pdf \\ -sDEVICE=pdfwrite \\ -sColorConversionStrategy=Gray \\ -dProcessColorModel=/DeviceGray \\ -dCompatibilityLevel=1.4 \\ -dAutoRotatePages=/None \\ -dNOPAUSE \\ -dBATCH \\ input.pdf 4.1.3 Split the pdf in smaller size to upload them on a ReMarkable Here we split the file in two: - The first one is composed of the first 120 pages - The second one will be the rest of the pages OUTPUT_DIR=res N_FILES=12 mkdir -p ${OUTPUT_DIR} for input in *.pdf do echo &quot;Treating ${input}&quot; total_pages=$(pdftk $input dump_data | grep NumberOfPages | awk &#39;{print $2}&#39;) chunk_size=$((total_pages / ${N_FILES})) start=1 for i in $(seq 1 ${N_FILES}); do end=$((start + chunk_size - 1)) if [ $i -eq ${N_FILES} ]; then end=$total_pages # Ensure the last chunk includes any remaining pages fi pdftk $input cat $start-$end output $(printf &quot;${OUTPUT_DIR}/${input}_part%02d.pdf&quot; &quot;$i&quot;) start=$((end + 1)) done done "],["setup-mail-and-mail-command-line.html", "4.2 Setup Mail and Mail Command Line", " 4.2 Setup Mail and Mail Command Line 4.2.0.1 Requirements sudo apt install msmtp msmtp-mta 4.2.0.2 Setup (example with Fastmail) Run msmtp --configure &lt;mail address&gt; and add the line password XXXXXXXXXX to the file ~/.msmtprc. The password should be created in the section “Connected apps &amp; API tokens” of Fastmail. Example for a self-hosted domain used with fastmail: account default host smtp.fastmail.com port 465 tls on tls_starttls off auth on user &lt;fastmail login mail&gt; from server@yourdomain.com password &lt;pass&gt; Change its rights: chmod 600 .msmtprc and check the symlink of sendmail: ls -la /usr/sbin/sendmail it should return: lrwxrwxrwx 1 root root 12 nov. 28 2016 /usr/sbin/sendmail -&gt; ../bin/msmtp Finally, make a test: echo -e &quot;Subject:Test\\n\\nIt Works&quot; | msmtp --from test@yourdomain.com &lt;destination_mail_address&gt; You need the -e to interpret the backslash You can also send html pages (this is sample.html): From: sender@mail.com To: recipient@mail.com Subject: This is the Subject Mime-Version: 1.0 Content-Type: text/html &lt;html&gt; &lt;head&gt;This is Email Head&lt;/head&gt; &lt;body&gt; &lt;h2&gt;This is the Main Title&lt;/h2&gt; &lt;p&gt;This is the body text&lt;/p&gt; &lt;/body&gt; &lt;/html&gt; then do $ cat sample.html | msmtp recipient@mail.com "],["movie-clips-blind-test.html", "4.3 Movie Clips Blind Test", " 4.3 Movie Clips Blind Test The goal of this game is to show short clips (~10 secondes) of movies and try to guess the movie list. 4.3.1 Solution 1 Cut every movies into short clips. Put these files into a folder and play files of this folder randomly using any video player. Script to split every movie into equal clips j=0 for i in *.mp4 do ffmpeg -i &quot;$i&quot; \\ -ss 600 \\ -t 3600 \\ -c copy \\ -map 0 \\ -segment_time 00:00:10 \\ -f segment \\ -reset_timestamps 1 res/${j}_%06d.mp4 ((j=j+1)) done Explanation: j=0 This line initializes a variable j to 0. This variable will be used to keep track of the segment number. for i in *.mp4 do This line starts a for loop that iterates over all the MP4 files in the current directory. ffmpeg -i &quot;$i&quot; \\ -ss 600 \\ -t 3600 \\ -c copy \\ -map 0 \\ -segment_time 00:00:10 \\ -f segment \\ -reset_timestamps 1 res/${j}_%06d.mp4 This line uses FFmpeg to split each input MP4 file into 10-second segments. Here’s what each option does: -i \"$i\" specifies the input file. -ss 600 specifies the start time in seconds. In this case, it starts at 10 minutes (600 seconds) into the video. -t 3600 specifies the duration in seconds. In this case, it extracts 1 hour (3600 seconds) of video. -c copy specifies that the video and audio codecs should be copied without re-encoding. -map 0 specifies that all streams from the input file should be included in the output. -segment_time 00:00:10 specifies the duration of each segment. -f segment specifies the output format as segmented MP4 files. -reset_timestamps 1 specifies that the timestamps of the output segments should be reset to zero. res/${j}_%06d.mp4 specifies the output file name pattern. %06d is a placeholder for the segment number, padded with leading zeros to 6 digits. ${j} is the current value of the variable j. The segments are saved in the res/ directory. ((j=j+1)) This line increments the j variable by 1. In summary, this script splits each MP4 video file in the current directory into 10-second segments starting from the 10th minute of the video and extracts 1 hour of video. The output segments are saved in the res/ directory with a filename pattern that includes the segment number. Plays the video randomly Vlc could be a good solution but between each video the screen flickers and we see the desktop. Instead we will use mpv: find . -iregex &quot;.*\\.\\(mp4\\|flv\\|MOV\\|webm\\|avi\\|mpg\\|mpeg\\)&quot; -type f -exec mpv --fs --shuffle --loop-playlist=inf &quot;{}&quot; + 4.3.2 Solution 2 (to be enhanced) We want to avoid recreating data so we will randomly play part of each movie programmatically using a python (3.10.6) script: import os import random from moviepy.video.io.VideoFileClip import VideoFileClip # Needs to be imported to be able to call the method `preview` from moviepy.editor import * # Set the path to the folder containing the movies path = &quot;.&quot; # Get a list of all the mp4 files in the folder files = [os.path.join(path, f) for f in os.listdir(path) if f.endswith(&quot;.mp4&quot;)] # Preload movie and its duration: data = {} for file in files: print(f&quot;Loading {file}&quot;) clip = VideoFileClip(file) duration = clip.duration data[file] = (clip, duration) # Loop through each movie file and play a random 10-second clip while True: # Choose a file randomly file = random.choice(files) # Load the video file clip clip, duration = data[file] # Set the start time of the clip to a random value between 0 and (duration - 10) start_time = random.uniform(0, duration - 10) # Set the end time of the clip to 10 seconds after the start time end_time = start_time + 10 # Extract the 10-second clip subclip = clip.subclip(start_time, end_time) # Display the clip subclip.preview(fullscreen=True) The requirements are: moviepy==1.0.3 pygame==2.4.0 "],["mpv-keyboard-shortcut-to-delete-file.html", "4.4 MPV Keyboard shortcut to delete file", " 4.4 MPV Keyboard shortcut to delete file The goal is to add a shortcut to mpv in order to soft delete the file you are watching. Create and edit the file ~/.config/mpv/scripts/delete-file.lua like this: local msg = require &#39;mp.msg&#39; local utils = require &#39;mp.utils&#39; function delete_current_file() local path = mp.get_property(&quot;path&quot;) if not path then mp.osd_message(&quot;No file currently playing.&quot;) return end -- Move the file to the trash using gio trash local res = utils.subprocess({args={&quot;gio&quot;, &quot;trash&quot;, path}}) if res.error or res.status ~= 0 then mp.osd_message(&quot;Failed to delete file: &quot; .. (res.error or &quot;unknown error&quot;)) msg.error(&quot;Failed to delete file: &quot; .. (res.error or &quot;unknown error&quot;)) else mp.osd_message(&quot;File moved to trash: &quot; .. path) msg.info(&quot;File moved to trash: &quot; .. path) mp.commandv(&quot;playlist-next&quot;, &quot;weak&quot;) end end mp.add_key_binding(&quot;D&quot;, &quot;delete-file&quot;, delete_current_file) "],["setup-server-with-self-hosted-services-using-docker.html", "4.5 Setup server with self-hosted services using Docker", " 4.5 Setup server with self-hosted services using Docker This tutorial works for an Ubuntu 24.04 LTS Server version on an OVH Eco server. 4.5.1 Setup SSH # To adapt export USERNAME=username export SSHPORT=1234 # Add user and give him sudo rights sudo adduser $USERNAME sudo usermod -aG sudo $USERNAME # Allow connection only on a specific port sudo ufw allow $SSHPORT/tcp # Allow ports for Nextcloud/Portainer/NPM sudo ufw allow 8080 sudo ufw allow 9443 sudo ufw allow 81 sudo ufw allow https sudo ufw enable Edit the file /etc/ssh/sshd_config (replace the variable manually): Port $SSHPORT PasswordAuthentication yes PermitEmptyPasswords no PermitRootLogin no PubkeyAuthentication no AllowUsers $USERNAME KbdInteractiveAuthentication yes # Change from no to yes Disable ssh.socket and Enable ssh.service Since ssh.socket can override custom port settings, it may be preventing SSH from listening on the correct port. To disable socket activation and rely only on the regular SSH service, do the following: sudo systemctl disable ssh.socket sudo systemctl stop ssh.socket sudo systemctl enable ssh sudo systemctl start ssh This ensures that SSH will run as a service and listen on your specified port directly. 4.5.2 Delete ubuntu user sudo userdel -f ubuntu 4.5.3 Setup dotfiles git clone https://github.com/tillwf/dotfiles cp dotfiles/home/.* . reset 4.5.4 Install Docker sudo apt update sudo apt install apt-transport-https ca-certificates curl software-properties-common curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg echo &quot;deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable&quot; | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null sudo apt update sudo apt install docker-ce docker-compose Give the user the rights to run docker: sudo usermod -aG docker ${USER} sudo systemctl restart docker Log out then log in or restart the server. 4.5.5 Install Portainer 4.5.5.1 Clone docker compose files git clone https://github.com/tillwf/self_hosted_stack and go to the folder portainer. Run the container docker-compose up -d Then go to this page https://ip-adress:9443/#!/init/admin and setup the password. Add the service to systemd As we want this service to be started whenever the server is restarted, we will add portainer as a service. Edit the file: sudo vim /etc/systemd/system/portainer.service like that: [Unit] Description=Portainer Requires=docker.service After=docker.service [Service] Restart=always User=&lt;username&gt; Group=docker WorkingDirectory=&lt;path to portainer docker compose&gt; # Shutdown container (if running) when unit is stopped ExecStartPre=/usr/bin/docker-compose -f docker-compose.yml down # Start container when unit is started ExecStart=/usr/bin/docker-compose -f docker-compose.yml up # Stop container when unit is stopped ExecStop=/usr/bin/docker-compose -f docker-compose.yml down [Install] WantedBy=multi-user.target and then run: sudo systemctl enable portainer.service sudo systemctl start portainer.service 4.5.5.2 Where to add a Stack 4.5.6 Install Ngnix Proxy Manager Create a new stack with the proper docker-compose.yml and the environment variables. Then setup your account by going to http://ip-adress:81 and change your credential. The default values are: admin@example.com changeme 4.5.6.1 Setup you subdomain As an example, for nextcloud, first you will need to create a A entry in to your DNS Zone nextcloud IN A &lt;ip adress&gt; Then in NPM Go to Hosts → Proxy hosts and create a new one: Tab Detail: Check Cache assets, block common exploits and web sockets support; Tab SSL: Force SSL, HTTP/2 and HSTS enabled; For Nextcloud, add the following lines to advanced tab, custom nginx configuration: proxy_set_header Host $host; proxy_set_header X-Forwarded-Proto $scheme; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_max_temp_file_size 16384m; client_max_body_size 0; location = /.well-known/carddav { return 301 $scheme://$host:$server_port/remote.php/dav; } location = /.well-known/caldav { return 301 $scheme://$host:$server_port/remote.php/dav; } 4.5.6.2 Setup SSL certificate (OVH) Go to this link to create token for a new application. Add permissions for GET, POST, PUT, and DELETE on the following paths: /domain/* /domain/zone/* This ensures the app can create, modify, and delete DNS TXT records for verification. dns_ovh_endpoint = ovh-eu dns_ovh_application_key = &lt;application_key&gt; dns_ovh_application_secret = &lt;application_secret&gt; dns_ovh_consumer_key = &lt;consumer_key&gt; 2024-11-25_18-12 4.5.7 Setup Nextcloud Use the Repository method to deploy your stack: 4.5.7.1 Change config.php Find the id of the nextcloud-nc image and run bash in it: docker ps docker exec -it &lt;container-name-or-id&gt; bash Edit the file /var/www/html/config/config.php: apt-get update apt-get install vim vim /var/www/html/config/config.php &#39;trusted_domains&#39; =&gt; array ( 0 =&gt; &#39;{nextcloud.domain.com}&#39;, ), &#39;redis&#39; =&gt; array ( &#39;host&#39; =&gt; &#39;redis&#39;, &#39;port&#39; =&gt; 6379, &#39;password&#39; =&gt; &#39;{redis_password}&#39;, ), &#39;filelocking.enabled&#39; =&gt; true, &#39;memcache.locking&#39; =&gt; &#39;\\OC\\Memcache\\Redis&#39;, &#39;trashbin_retention_obligation&#39; =&gt; &#39;30, 60&#39;, &#39;overwriteprotocol&#39; =&gt; &#39;https&#39;, # Only when https is enabled &#39;log_type&#39; =&gt; &#39;file&#39;, &#39;logfile&#39; =&gt; &#39;nextcloud.log&#39;, 4.5.7.2 Setup the data folder If you want to have access to your data folder in your HOME do: cd $HOME mkdir nextcloud_data touch nextcloud_data/.ocdata sudo chown -R www-data:www-data nextcloud_data sudo chmod -R 0770 nextcloud_data Setup Cron for the news app For the news app, to enable the cron update add this line to your local crontab: /usr/bin/docker exec -u www-data nextcloud-nc-1 php -f /var/www/html/cron.php Install dependencies for extensions Adding local external storage requires smbclient and to use grauphel you need the oauth package of php. To install those requirements during the deployement of the container you need to add a Dockerfile in the folder of your docker compose which will be like this: FROM nextcloud:apache RUN apt-get update; \\ apt-get install -y procps smbclient libsmbclient-dev; \\ pecl install smbclient; \\ docker-php-ext-enable smbclient; \\ apt install -y libpcre3-dev; \\ pecl install oauth; \\ docker-php-ext-enable oauth; \\ rm -rf /var/lib/apt/lists/* As this file specify the image used (nexcloud:apache) you have to remove the image line in your docker-compose.yml otherwise it will use the version of the docker compose and will not apply the Dockerfile. 4.5.8 FAQ How to completetely clear one of your container Print current docker images: docker images Remove the wanted images docker rmi &lt;image_name&gt; List the docker volumes docker volume ls Remove the wanted volume docker volume rm &lt;volume_name&gt; Kill everything: removecontainers() { docker stop $(docker ps -aq) docker rm $(docker ps -aq) } armageddon() { removecontainers docker network prune -f docker rmi -f $(docker images --filter dangling=true -qa) docker volume rm $(docker volume ls --filter dangling=true -q) docker rmi -f $(docker images -qa) } 4.5.9 Ressources Initial server setup with Ubuntu 22.04 How to install and use Docker on Ubuntu 22.04 Setup Nextcloud with Portainer and nginx proxy manager Install Grauphel "],["spotify-job-watcher.html", "4.6 Spotify Job Watcher", " 4.6 Spotify Job Watcher 4.6.1 Find the job URL Go to https://www.lifeatspotify.com/jobs and make your search with the Inspector open. Copy the search URL and put it in the script below. 4.6.2 Setup mail in command line Setup Mail and Mail Command Line 4.6.3 Script to check job changes You can now fill the &lt;search_url&gt; and the &lt;destination_mail&gt; in the script below and put the script in a cron for example. Example of search url: https://api-dot-new-spotifyjobs-com.nw.r.appspot.com/wp-json/animal/v1/job/search?l=london%2Cstockholm%2Cremote-emea&amp;c=machine-learning%2Cdata-science%2Cdata #!/bin/bash # TO FILL API_URL=&lt;search_url&gt; DESTINATION_MAIL=&lt;destination_mail&gt; # Define file paths jobs_file=&quot;$HOME/jobs.txt&quot; new_jobs_file=&quot;$HOME/new_jobs.txt&quot; diff_file=&quot;$HOME/diff_job.txt&quot; header_file=&quot;$HOME/header.html&quot; footer_file=&quot;$HOME/footer.html&quot; # Function to fetch and process API data get_and_process_data() { curl -sSL -H &#39;User-Agent: Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/112.0&#39; \\ -H &#39;Accept-Language: en-US,en;q=0.5&#39; \\ -H &#39;Accept: application/json, text/plain, */*&#39; \\ -H &#39;Origin: https://lifeatspotify.com&#39; \\ -H &#39;Connection: keep-alive&#39; \\ -H &#39;Referer: https://lifeatspotify.com/&#39; \\ -H &#39;Sec-Fetch-Dest: empty&#39; \\ -H &#39;Sec-Fetch-Mode: cors&#39; \\ -H &#39;Sec-Fetch-Site: cross-site&#39; &quot;$API_URL&quot; \\ | jq -r &#39;.result[].text&#39; \\ | sort &gt; &quot;$new_jobs_file&quot; } # Function to create diff file create_diff_file() { diff &quot;$jobs_file&quot; &quot;$new_jobs_file&quot; | awk &#39;/&gt;/ { sub(/^&gt;/, &quot;&quot;); print }&#39; &gt; &quot;$diff_file&quot; } # Function to send email if diff file is not empty send_email() { if [ -s &quot;$diff_file&quot; ]; then ( echo &quot;&lt;ul&gt;&quot;; sed &#39;s/^/&lt;li&gt; /; s/$/ &lt;\\/li&gt;/&#39; &quot;$diff_file&quot;; echo &quot;&lt;/ul&gt;&quot; ) &gt; &quot;$diff_file.tmp&quot; mv &quot;$diff_file.tmp&quot; &quot;$diff_file&quot; cat &quot;$header_file&quot; &quot;$diff_file&quot; &quot;$footer_file&quot; | msmtp $DESTINATION_MAIL fi } # Main script execution touch &quot;$jobs_file&quot; get_and_process_data if ! diff -q &quot;$jobs_file&quot; &quot;$new_jobs_file&quot; &gt;/dev/null; then create_diff_file send_email cp &quot;$new_jobs_file&quot; &quot;$jobs_file&quot; fi then chmod +x spotify.sh Here are examples of header.html and footer.html: From: &lt;from mail address&gt; To: &lt;to mail address&gt; Subject: New Spotify Jobs Mime-Version: 1.0 Content-Type: text/html &lt;!DOCTYPE html&gt; &lt;html lang=&quot;en&quot;&gt; &lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0&quot;&gt; &lt;title&gt;Spotify Job Watcher Alert&lt;/title&gt; &lt;style&gt; body { font-family: Arial, sans-serif; color: #333; margin: 20px; } h1 { color: #1DB954; } &lt;/style&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt;Spotify Job Watcher Alert&lt;/h1&gt; &lt;p&gt;The following new job listings have been posted:&lt;/p&gt; &lt;p&gt;Best regards,&lt;br&gt;Your Job Watcher Script&lt;/p&gt; &lt;/body&gt; &lt;/html&gt; "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
