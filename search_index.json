[["index.html", "Table of contents", " Table of contents Data Engineering Machine Learning Miscellaneous "],["data-engineering.html", "1 Data Engineering", " 1 Data Engineering "],["how-to-deploy-an-huggingface-model-api-on-google-cloud-function.html", "1.1 How to deploy an HuggingFace model API on Google Cloud Function", " 1.1 How to deploy an HuggingFace model API on Google Cloud Function Create a file main.py like that: from transformers import TFBertForSequenceClassification from transformers import AutoTokenizer import tensorflow as tf model = None tokenizer = None def predict(request): global model global tokenizer if model is None: model = TFBertForSequenceClassification.from_pretrained(&quot;jonaskoenig/xtremedistil-l6-h256-uncased-question-vs-statement-classifier&quot;, cache_dir=&quot;/tmp&quot;) tokenizer = AutoTokenizer.from_pretrained(&quot;jonaskoenig/xtremedistil-l6-h256-uncased-question-vs-statement-classifier&quot;, cache_dir=&quot;/tmp&quot;) params = request.get_json() sentence = params[&quot;description&quot;] predictions = model(**tokenizer(sentence, return_tensors=&quot;tf&quot;)) probabilities = tf.nn.softmax(predictions.logits[0], axis=0).numpy() return { &quot;no_question&quot;: float(probabilities[0]), &quot;question&quot;: float(probabilities[1]) } a file requirements.txt like that: tensorflow==2.12.0 transformers==4.27.4 a Dockerfile like that: FROM python:3.10.6 ENV PYTHONUNBUFFERED 1 EXPOSE 8000 WORKDIR /app COPY requirements.txt ./ RUN pip install --upgrade pip &amp;&amp; \\ pip install -r requirements.txt COPY . ./ ENV PYTHONPATH huggingfastapi CMD [&quot;uvicorn&quot;, &quot;main:app&quot;, &quot;--host&quot;, &quot;0.0.0.0&quot;, &quot;--port&quot;, &quot;8000&quot;] You can now build your image and push it to your registry: docker build -t &lt;image_url&gt; -f Dockerfile . docker push &lt;image_url&gt; and deploy it on Google Cloud Run: gcloud run deploy &lt;image_name&gt; --image &lt;image_url&gt;:latest --region europe-west1 --port 8000 --memory 4Gi "],["filter-in-join-vs-filter-in-where.html", "1.2 Filter in JOIN vs filter in WHERE", " 1.2 Filter in JOIN vs filter in WHERE Init tables library(DBI) db = dbConnect(RSQLite::SQLite(), dbname = &quot;db.sqlite&quot;) CREATE TABLE users AS SELECT 1 AS user_id, DATETIME(&quot;2023-01-01&quot;) AS created_at UNION ALL SELECT 2 AS user_id, DATETIME(&quot;2023-01-02&quot;) AS created_at UNION ALL SELECT 3 AS user_id, DATETIME(&quot;2023-01-03&quot;) AS created_at; CREATE TABLE trackers AS SELECT 1 AS user_id, 1 AS trackable_id, DATETIME(&quot;2023-01-04&quot;) AS created_at UNION ALL SELECT 2 AS user_id, 1 AS trackable_id, DATETIME(&quot;2023-01-02&quot;) AS created_at UNION ALL SELECT 2 AS user_id, 1 AS trackable_id, DATETIME(&quot;2023-01-02&quot;) AS created_at; SELECT * FROM users; Table 1.1: 3 records user_id created_at 1 2023-01-01 00:00:00 2 2023-01-02 00:00:00 3 2023-01-03 00:00:00 SELECT * FROM trackers; Table 1.2: 3 records user_id trackable_id created_at 1 1 2023-01-04 00:00:00 2 1 2023-01-02 00:00:00 2 1 2023-01-02 00:00:00 Simple JOIN SELECT * FROM users u LEFT JOIN trackers t ON u.user_id = t.user_id Table 1.3: 4 records user_id created_at user_id trackable_id created_at 1 2023-01-01 00:00:00 1 1 2023-01-04 00:00:00 2 2023-01-02 00:00:00 2 1 2023-01-02 00:00:00 2 2023-01-02 00:00:00 2 1 2023-01-02 00:00:00 3 2023-01-03 00:00:00 NA NA NA Filter in the WHERE clause SELECT * FROM users u LEFT JOIN trackers t ON u.user_id = t.user_id WHERE DATE(t.created_at) &gt;= DATE(&quot;2023-01-05&quot;, &quot;-10 DAY&quot;) Table 1.4: 3 records user_id created_at user_id trackable_id created_at 1 2023-01-01 00:00:00 1 1 2023-01-04 00:00:00 2 2023-01-02 00:00:00 2 1 2023-01-02 00:00:00 2 2023-01-02 00:00:00 2 1 2023-01-02 00:00:00 You lose the user_id=3 Filter in the JOIN clause SELECT * FROM users u LEFT JOIN trackers t ON u.user_id = t.user_id AND DATE(t.created_at) &gt;= DATE(&quot;2023-01-05&quot;, &quot;-10 DAY&quot;) Table 1.5: 4 records user_id created_at user_id trackable_id created_at 1 2023-01-01 00:00:00 1 1 2023-01-04 00:00:00 2 2023-01-02 00:00:00 2 1 2023-01-02 00:00:00 2 2023-01-02 00:00:00 2 1 2023-01-02 00:00:00 3 2023-01-03 00:00:00 NA NA NA Adding more filters in the JOIN clause SELECT * FROM users u LEFT JOIN trackers t ON u.user_id = t.user_id AND DATE(t.created_at) &gt;= DATE(&quot;2023-01-05&quot;, &quot;-10 DAY&quot;) AND t.created_at BETWEEN DATETIME(u.created_at, &quot;+ 24 HOUR&quot;) AND DATETIME(u.created_at, &quot;+ 48 HOUR&quot;) Table 1.6: 3 records user_id created_at user_id trackable_id created_at 1 2023-01-01 00:00:00 NA NA NA 2 2023-01-02 00:00:00 NA NA NA 3 2023-01-03 00:00:00 NA NA NA Then you can make an accurate count SELECT u.user_id, CAST(COUNT(DISTINCT t.trackable_id) &gt; 0 AS INT) AS has_action FROM users u LEFT JOIN trackers t ON u.user_id = t.user_id AND DATE(t.created_at) &gt;= DATE(&quot;2023-01-05&quot;, &quot;-10 DAY&quot;) AND t.created_at BETWEEN DATETIME(u.created_at, &quot;+ 24 HOUR&quot;) AND DATETIME(u.created_at, &quot;+ 48 HOUR&quot;) GROUP BY u.user_id Table 1.7: 3 records user_id has_action 1 0 2 0 3 0 If we filter on the users in the JOIN clause SELECT u.user_id, CAST(COUNT(DISTINCT t.trackable_id) &gt; 0 AS INT) AS has_action FROM users u LEFT JOIN trackers t ON u.user_id = t.user_id AND DATE(t.created_at) &gt;= DATE(&quot;2023-01-05&quot;, &quot;-10 DAY&quot;) AND t.created_at BETWEEN DATETIME(u.created_at, &quot;+ 24 HOUR&quot;) AND DATETIME(u.created_at, &quot;+ 48 HOUR&quot;) AND u.user_id = 3 GROUP BY u.user_id Table 1.8: 3 records user_id has_action 1 0 2 0 3 0 If we filter on the users in the WHERE clause SELECT u.user_id, CAST(COUNT(DISTINCT t.trackable_id) &gt; 0 AS INT) AS has_action FROM users u LEFT JOIN trackers t ON u.user_id = t.user_id AND DATE(t.created_at) &gt;= DATE(&quot;2023-01-05&quot;, &quot;-10 DAY&quot;) AND t.created_at BETWEEN DATETIME(u.created_at, &quot;+ 24 HOUR&quot;) AND DATETIME(u.created_at, &quot;+ 48 HOUR&quot;) WHERE u.created_at &gt;= &quot;2023-01-03&quot; GROUP BY u.user_id Table 1.9: 1 records user_id has_action 3 0 "],["machine-learning.html", "2 Machine Learning", " 2 Machine Learning "],["ab-testing.html", "2.1 A/B Testing", " 2.1 A/B Testing "],["xgboost.html", "2.2 XGBoost", " 2.2 XGBoost "],["miscellaneous.html", "3 Miscellaneous", " 3 Miscellaneous "],["convert-cbz-files-to-pdf.html", "3.1 Convert CBZ files to PDF", " 3.1 Convert CBZ files to PDF 3.1.1 Convertion Install mutool command line: sudo apt install mupdf-tools Then to convert every cbz file of a folder do: for i in *.cbz do echo &quot;$i&quot; mutool convert -O compress -o &quot;`basename &quot;$i&quot; .cbz`&quot;.pdf &quot;$i&quot; done The -0 compress is there to reduce the pdf final size. If you want to find every cbz file recursively and convert them do: find . -name &quot;*.cbz&quot; -exec mutool convert -O compress -o {}.pdf {} \\; or if you want to remove the .cbz: find . -name &quot;*.cbz&quot; -exec sh -c &#39;mutool convert -O compress -o &quot;$(basename &quot;{}&quot; .cbz)&quot;.pdf &quot;{}&quot;&#39; \\ 3.1.2 Convert the pdf to grayscale to reduce its size gs \\ -sOutputFile=output.pdf \\ -sDEVICE=pdfwrite \\ -sColorConversionStrategy=Gray \\ -dProcessColorModel=/DeviceGray \\ -dCompatibilityLevel=1.4 \\ -dAutoRotatePages=/None \\ -dNOPAUSE \\ -dBATCH \\ input.pdf 3.1.3 Split the pdf in smaller size to upload them on a ReMarkable Here we split the file in two: - The first one is composed of the first 120 pages - The second one will be the rest of the pages for i in *.pdf do OUTPUT_NAME=$(basename &quot;$i&quot; .pdf) echo $OUTPUT_NAME pdftk &quot;$i&quot; cat 1-120 output res/${OUTPUT_NAME}_1.pdf pdftk &quot;$i&quot; cat 121-end output res/${OUTPUT_NAME}_2.pdf done "],["setup-mail-and-mail-command-line.html", "3.2 Setup Mail and Mail Command Line", " 3.2 Setup Mail and Mail Command Line 3.2.0.1 Requirements sudo apt install msmtp msmtp-mta 3.2.0.2 Setup (example with Fastmail) Run msmtp --configure and add the line password XXXXXXXXXX to the file ~/.msmtprc. Example for a self-hosted domain used with fastmail: account default host smtp.fastmail.com port 465 tls on tls_starttls off auth on user &lt;fastmail login mail&gt; from server@yourdomain.com password &lt;pass&gt; Change its rights: chmod 600 .msmtprc and check the symlink of sendmail: ls -la /usr/sbin/sendmail it should return: lrwxrwxrwx 1 root root 12 nov. 28 2016 /usr/sbin/sendmail -&gt; ../bin/msmtp Finally, make a test: echo -e &quot;Subject:Test\\n\\nIt Works&quot; | msmtp --from test@yourdomain.com &lt;destination_mail_address&gt; You need the -e to interpret the backslash You can also send html pages (this is sample.html): From: sender@mail.com To: recipient@mail.com Subject: This is the Subject Mime-Version: 1.0 Content-Type: text/html &lt;html&gt; &lt;head&gt;This is Email Head&lt;/head&gt; &lt;body&gt; &lt;h2&gt;This is the Main Title&lt;/h2&gt; &lt;p&gt;This is the body text&lt;/p&gt; &lt;/body&gt; &lt;/html&gt; then do $ cat sample.html | msmtp recipient@mail.com "],["movie-clips-blind-test.html", "3.3 Movie Clips Blind Test", " 3.3 Movie Clips Blind Test The goal of this game is to show short clips (~10 secondes) of movies and try to guess the movie list. 3.3.1 Solution 1 Cut every movies into short clips. Put these files into a folder and play files of this folder randomly using any video player. Script to split every movie into equal clips j=0 for i in *.mp4 do ffmpeg -i &quot;$i&quot; \\ -ss 600 \\ -t 3600 \\ -c copy \\ -map 0 \\ -segment_time 00:00:10 \\ -f segment \\ -reset_timestamps 1 res/${j}_%06d.mp4 ((j=j+1)) done Explanation: j=0 This line initializes a variable j to 0. This variable will be used to keep track of the segment number. for i in *.mp4 do This line starts a for loop that iterates over all the MP4 files in the current directory. ffmpeg -i &quot;$i&quot; \\ -ss 600 \\ -t 3600 \\ -c copy \\ -map 0 \\ -segment_time 00:00:10 \\ -f segment \\ -reset_timestamps 1 res/${j}_%06d.mp4 This line uses FFmpeg to split each input MP4 file into 10-second segments. Here’s what each option does: -i \"$i\" specifies the input file. -ss 600 specifies the start time in seconds. In this case, it starts at 10 minutes (600 seconds) into the video. -t 3600 specifies the duration in seconds. In this case, it extracts 1 hour (3600 seconds) of video. -c copy specifies that the video and audio codecs should be copied without re-encoding. -map 0 specifies that all streams from the input file should be included in the output. -segment_time 00:00:10 specifies the duration of each segment. -f segment specifies the output format as segmented MP4 files. -reset_timestamps 1 specifies that the timestamps of the output segments should be reset to zero. res/${j}_%06d.mp4 specifies the output file name pattern. %06d is a placeholder for the segment number, padded with leading zeros to 6 digits. ${j} is the current value of the variable j. The segments are saved in the res/ directory. ((j=j+1)) This line increments the j variable by 1. In summary, this script splits each MP4 video file in the current directory into 10-second segments starting from the 10th minute of the video and extracts 1 hour of video. The output segments are saved in the res/ directory with a filename pattern that includes the segment number. Plays the video randomly Vlc could be a good solution but between each video the screen flickers and we see the desktop. Instead we will use mpv: find . -iregex &quot;.*\\.\\(mp4\\|flv\\|MOV\\|webm\\|avi\\|mpg\\|mpeg\\)&quot; -type f -exec mpv --fs --shuffle --loop-playlist=inf &quot;{}&quot; + 3.3.2 Solution 2 (to be enhanced) We want to avoid recreating data so we will randomly play part of each movie programmatically using a python (3.10.6) script: import os import random from moviepy.video.io.VideoFileClip import VideoFileClip # Needs to be imported to be able to call the method `preview` from moviepy.editor import * # Set the path to the folder containing the movies path = &quot;.&quot; # Get a list of all the mp4 files in the folder files = [os.path.join(path, f) for f in os.listdir(path) if f.endswith(&quot;.mp4&quot;)] # Preload movie and its duration: data = {} for file in files: print(f&quot;Loading {file}&quot;) clip = VideoFileClip(file) duration = clip.duration data[file] = (clip, duration) # Loop through each movie file and play a random 10-second clip while True: # Choose a file randomly file = random.choice(files) # Load the video file clip clip, duration = data[file] # Set the start time of the clip to a random value between 0 and (duration - 10) start_time = random.uniform(0, duration - 10) # Set the end time of the clip to 10 seconds after the start time end_time = start_time + 10 # Extract the 10-second clip subclip = clip.subclip(start_time, end_time) # Display the clip subclip.preview(fullscreen=True) The requirements are: moviepy==1.0.3 pygame==2.4.0 "],["movie-clips-blind-test-1.html", "3.4 Movie Clips Blind Test", " 3.4 Movie Clips Blind Test The goal is to add a shortcut to mpv in order to soft delete the file you are watching. Create and edit the file ~/.config/mpv/scripts/delete-file.lua like this: local msg = require &#39;mp.msg&#39; local utils = require &#39;mp.utils&#39; function delete_current_file() local path = mp.get_property(&quot;path&quot;) if not path then mp.osd_message(&quot;No file currently playing.&quot;) return end -- Move the file to the trash using gio trash local res = utils.subprocess({args={&quot;gio&quot;, &quot;trash&quot;, path}}) if res.error or res.status ~= 0 then mp.osd_message(&quot;Failed to delete file: &quot; .. (res.error or &quot;unknown error&quot;)) msg.error(&quot;Failed to delete file: &quot; .. (res.error or &quot;unknown error&quot;)) else mp.osd_message(&quot;File moved to trash: &quot; .. path) msg.info(&quot;File moved to trash: &quot; .. path) mp.commandv(&quot;playlist-next&quot;, &quot;weak&quot;) end end mp.add_key_binding(&quot;D&quot;, &quot;delete-file&quot;, delete_current_file) "],["setup-server-with-self-hosted-services-using-docker.html", "3.5 Setup server with self-hosted services using Docker", " 3.5 Setup server with self-hosted services using Docker This tutorial works for an Ubuntu 22.04 LTS Server version on an OVH Eco server. 3.5.1 Setup SSH # To adapt export USERNAME=username export SSHPORT=1234 # Add user and give him sudo rights sudo adduser $USERNAME sudo usermod -aG sudo $USERNAME # Allow connection only on a specific port sudo ufw allow $SSHPORT/tcp # Allow ports for Nextcloud/Portainer/NPM sudo ufw allow 8080 sudo ufw allow 9443 sudo ufw allow 81 sudo ufw allow https sudo ufw enable Edit the file /etc/ssh/sshd_config (replace the variable manually): Port $SSHPORT PasswordAuthentication yes PermitEmptyPasswords no PermitRootLogin no AllowUsers $USERNAME Restart the service ssh: service ssh restart 3.5.2 Delete ubuntu user sudo userdel -f ubuntu 3.5.3 Setup dotfiles git clone https://github.com/tillwf/dotfiles cp dotfiles/home/* . reset 3.5.4 Install Docker sudo apt update sudo apt install apt-transport-https ca-certificates curl software-properties-common curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg echo &quot;deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable&quot; | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null sudo apt update sudo apt install docker-ce docker-compose Give the user the rights to run docker: sudo usermod -aG docker $USERNAME sudo - $USERNAME 3.5.5 Install Portainer 3.5.5.1 Clone docker compose files git clone https://github.com/tillwf/self-hosted_docker_setups and go to the folder portainer. Run the container docker-compose up -d Then go to this page https://ip-adress:9443/#!/init/admin and setup the password. Add the service to systemd As we want this service to be started whenever the server is restarted, we will add portainer as a service. Edit the file: sudo vim /etc/systemd/system/portainer.service like that: [Unit] Description=Portainer Requires=docker.service After=docker.service [Service] Restart=always User=&lt;username&gt; Group=docker WorkingDirectory=&lt;path to portainer docker compose&gt; # Shutdown container (if running) when unit is stopped ExecStartPre=/usr/bin/docker-compose -f docker-compose.yml down # Start container when unit is started ExecStart=/usr/bin/docker-compose -f docker-compose.yml up # Stop container when unit is stopped ExecStop=/usr/bin/docker-compose -f docker-compose.yml down [Install] WantedBy=multi-user.target and then run: sudo systemctl enable portainer.service sudo systemctl start portainer.service 3.5.5.2 Where to add a Stack 3.5.6 Install Ngnix Proxy Manager Create a new stack with the proper docker-compose.yml and the environment variables. Then setup your account by going to http://ip-adress:81 and change your credential. The default values are: admin@example.com changeme 3.5.6.1 Setup you subdomain As an example, for nextcloud, first you will need to create a A entry in to your DNS Zone nextcloud IN A &lt;ip adress&gt; Then in NPM Go to Hosts → Proxy hosts and create a new one: Tab Detail: Check Cache assets, block common exploits and web sockets support; Tab SSL: Force SSL, HTTP/2 and HSTS enabled; For Nextcloud, add the following lines to advanced tab, custom nginx configuration: proxy_set_header Host $host; proxy_set_header X-Forwarded-Proto $scheme; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_max_temp_file_size 16384m; client_max_body_size 0; location = /.well-known/carddav { return 301 $scheme://$host:$server_port/remote.php/dav; } location = /.well-known/caldav { return 301 $scheme://$host:$server_port/remote.php/dav; } 3.5.7 Setup Nextcloud 3.5.7.1 Change config.php &#39;redis&#39; =&gt; array ( &#39;host&#39; =&gt; &#39;redis&#39;, &#39;port&#39; =&gt; 6379, &#39;password&#39; =&gt; &#39;{redis_password}&#39;, ), &#39;filelocking.enabled&#39; =&gt; true, &#39;memcache.locking&#39; =&gt; &#39;\\OC\\Memcache\\Redis&#39;, &#39;trashbin_retention_obligation&#39; =&gt; &#39;30, 60&#39;, &#39;overwriteprotocol&#39; =&gt; &#39;https&#39;, # Only when https is enabled &#39;log_type&#39; =&gt; &#39;file&#39;, &#39;logfile&#39; =&gt; &#39;nextcloud.log&#39;, and add you subdomain in the truted_domains list. 3.5.7.2 Setup the data folder If you want to have access to your data folder in your HOME do: cd $HOME mkdir nextcloud_data touch nextcloud_data/.ocdata sudo chown -R www-data:www-data nextcloud_data sudo chmod -R 0770 nextcloud_data Setup Cron for the news app For the news app, to enable the cron update add this line to your local crontab: /usr/bin/docker exec -u www-data nextcloud-nc-1 php -f /var/www/html/cron.php Install dependencies for extensions Adding local external storage requires smbclient and to use grauphel you need the oauth package of php. To install those requirements during the deployement of the container you need to add a Dockerfile in the folder of your docker compose which will be like this: FROM nextcloud:apache RUN apt-get update; \\ apt-get install -y procps smbclient libsmbclient-dev; \\ pecl install smbclient; \\ docker-php-ext-enable smbclient; \\ apt install -y libpcre3-dev; \\ pecl install oauth; \\ docker-php-ext-enable oauth; \\ rm -rf /var/lib/apt/lists/* As this file specify the image used (nexcloud:apache) you have to remove the image line in your docker-compose.yml otherwise it will use the version of the docker compose and will not apply the Dockerfile. Use the Repository method to deploy your stack: 3.5.8 FAQ How to completetely clear one of your container Print current docker images: docker images Remove the wanted images docker rmi &lt;image_name&gt; List the docker volumes docker volume ls Remove the wanted volume docker volume rm &lt;volume_name&gt; Kill everything: removecontainers() { docker stop $(docker ps -aq) docker rm $(docker ps -aq) } armageddon() { removecontainers docker network prune -f docker rmi -f $(docker images --filter dangling=true -qa) docker volume rm $(docker volume ls --filter dangling=true -q) docker rmi -f $(docker images -qa) } 3.5.9 Ressources Initial server setup with Ubuntu 22.04 How to install and use Docker on Ubuntu 22.04 Setup Nextcloud with Portainer and nginx proxy manager Install Grauphel "],["spotify-job-watcher.html", "3.6 Spotify Job Watcher", " 3.6 Spotify Job Watcher 3.6.1 Find the job URL Go to https://www.lifeatspotify.com/jobs and make your search with the Inspector open. Copy the search URL and put it in the script below. 3.6.2 Setup mail in command line Setup Mail and Mail Command Line 3.6.3 Script to check job changes You can now fill the &lt;search_url&gt; and the &lt;destination_mail&gt; in the script below and put the script in a cron for example. #!/bin/bash # Define file paths jobs_file=&quot;$USER/jobs.txt&quot; new_jobs_file=&quot;$USER/new_jobs.txt&quot; diff_file=&quot;$USER/diff_job.txt&quot; header_file=&quot;$USER/header.html&quot; footer_file=&quot;$USER/footer.html&quot; # API URL URL=&lt;search_url&gt; # Function to fetch and process API data get_and_process_data() { curl -sSL -H &#39;User-Agent: Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/112.0&#39; \\ -H &#39;Accept-Language: en-US,en;q=0.5&#39; \\ -H &#39;Accept: application/json, text/plain, */*&#39; \\ -H &#39;Origin: https://lifeatspotify.com&#39; \\ -H &#39;Connection: keep-alive&#39; \\ -H &#39;Referer: https://lifeatspotify.com/&#39; \\ -H &#39;Sec-Fetch-Dest: empty&#39; \\ -H &#39;Sec-Fetch-Mode: cors&#39; \\ -H &#39;Sec-Fetch-Site: cross-site&#39; &quot;$URL&quot; \\ | jq -r &#39;.result[].text&#39; \\ | sort &gt; &quot;$new_jobs_file&quot; } # Function to create diff file create_diff_file() { diff &quot;$jobs_file&quot; &quot;$new_jobs_file&quot; | awk &#39;/&gt;/ { sub(/^&gt;/, &quot;&quot;); print }&#39; &gt; &quot;$diff_file&quot; } # Function to send email if diff file is not empty send_email() { if [ -s &quot;$diff_file&quot; ]; then ( echo &quot;&lt;ul&gt;&quot;; sed &#39;s/^/&lt;li&gt; /; s/$/ &lt;\\/li&gt;/&#39; &quot;$diff_file&quot;; echo &quot;&lt;/ul&gt;&quot; ) &gt; &quot;$diff_file.tmp&quot; mv &quot;$diff_file.tmp&quot; &quot;$diff_file&quot; cat &quot;$header_file&quot; &quot;$diff_file&quot; &quot;$footer_file&quot; | msmtp &lt;destination_mail&gt; fi } # Main script execution touch &quot;$jobs_file&quot; get_and_process_data if ! diff -q &quot;$jobs_file&quot; &quot;$new_jobs_file&quot; &gt;/dev/null; then create_diff_file send_email cp &quot;$new_jobs_file&quot; &quot;$jobs_file&quot; fi "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
